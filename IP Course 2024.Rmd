---
title: "IP Course 2024"
output: 
  html_document:
    toc: true
    toc_depth: 2
    toc_float: true
    theme: !expr sample(c("yeti", "united", "lumen"), 1)
date: "2024-05-10"

---
## Getting ready

Load all dependencies that are required for this exercise. The following code will automatically install all the dependency that are not already present in your own computer.

```{r, echo=T}
if (!require("pacman")) 
  install.packages("pacman")
pacman::p_load(car, reshape2, plyr, ggplot2, Hmisc, multcomp, colorDF)
```

Let's start by uploading our data set. Make sure to specify your directory properly

```{r, echo=T}
input = "https://raw.githubusercontent.com/MarcoLF/IP_course_2024/main/IP%20course.csv"
IP = as.data.frame(read.csv(input))
IP$Plate = as.factor(IP$Plate)
IP$Site = as.factor(IP$Site)
```
and first of all let’s have a look at the data.

```{r, echo=T}
head(IP) #it shows by default the first 6 rows of the data set
```

```{r, echo=T}
str(IP) #it shows the internal structure of the data set
```

his exercise focuses on the realization of Fig 7 (ref to Labnote II_2023.docx). With this specific figure we want to show all the differences found about the AB resistance across sites. It is usually a good practice to first sketch the final graph we want to obtain. In this specific case, we aim to represent the CFU that we measured as a function of both plate type (w/o AB, and AB type) and the sampling site (A or B). We are essentially referring to a simple model...

## the model
that is
```{r, echo=T}
model = aov(Log.CFU.gram ~ Plate + Site, data = IP) #note that we are already using the function for anova
```
However, we are also interested on whether the interaction between the two independent variables Plate and Site could explain possible differences. In other words we need to expand our model as
```{r, echo=T}
model = aov(Log.CFU.gram ~ Plate + Site + Plate:Site, data = IP)
```
*Note* - **In R, the above syntax is identical to Log.CFU.gram ~ Plate*Site**

*Note* - **the model’s output contains a list of different arguments. To visualize them, just type: ls(model)**

Here, we use a *two-way ANOVA* to answer our question. Thus, it could be relevant to first check whether variances are homogeneous and data normally distributed. We can first compute *Residulas vs Fitted* values calculated from the model.
```{r, echo=T}
plot(model, 1)
```
among all the data, the above graph shows that there are three data points (16, 18, 19; numbers refer to the raw entry in the data frame) that are actually considered to be outliers. In some instance, it is fine to remove those data as long as it is acknowledge to the reader. However, let’s first test formally for the homogeneity of the variances using the Levene’s test.
```{r, echo=T}
leveneTest(Log.CFU.gram ~ Plate*Site, data = IP)
```
The **Levene’s test** result in a **p-value = 0.83** which accepts the null-hypothesis. Thus, variances could be considered homogeneous.

*Note* - **In the previous command line I specify the model for clarity. Yet, we could have directly refer to the model using: levenTest(model)**

Let’s now check for normality. We do so by looking at the model’s residual distribution. First, we generate a graph that can already gives us a first picture on whether our data are normally distributed.
```{r, echo=T}
plot(model, 2)
```
Already from the *Q-Q plot* we can see that the data are not very normally distributed. We can test this more formally by firstly extracting the actual residulas from our model and then using the Shapiro test for normality.
```{r, echo=T}
resid = residuals(object = model)
shapiro.test(x = resid)
```
**Shapiro test** confirmed our expectation and rejected the null-hypothesis for normality. This is most likely associated with the low sampling number. However, ANOVA is not strongly affected by deviation from normality **(Harwell et al 1992, Lix et al. 1996).**

## ggplot
Now it could be useful to generate the graph we are interested in. We do
so by using the ggplot package. You can find
**[online](https://ggplot2.tidyverse.org/)** a large number of tutorials
explaining how this package works (for beginners and advanced users).
Here, I try to guide you through the vary basic concepts of *ggplot*
that are relevant to our aim. If you already know how to use it, you can
skip this specific part. To use **ggplot** you first need to define the
parameters you want to graph. Thus, you need to specify the source of
your data **data** (e.g., where ggplot needs to get the data from), and
then define what in ggplot are called *aesthetics*, a fancy way to say x
and y axis (and more). In our case, the data corresponds to the data
frame named **IP** and the aesthetics are **Log.CFU.gram** and
**Plate**.
```{r, echo=T}
ggplot(data = IP, aes(Plate, Log.CFU.gram)) #this is the base from where one starts to build the actual graph
```

Before plotting our data onto this gray area, there is an issue that
maybe you have already noticed. At the beginning we mentioned *two*
independent variables, but here, with this graph, we only have one which
is **Plate**. How can we introduce the second variable that is the
sampling **Site**? An easy way to do it is by coloring our data
accordingly. We can expand the *aesthetics* with **col**.

```{r, echo=T}
ggplot(data = IP, aes(Plate, Log.CFU.gram, col = Site)) #this is the base from where one starts to build the actual graph
```
Amazing, still gray. Well, now we have all the bases and we can start to
build our graph for real finally. To do so, we need to concatenate the
first function *ggplot()* to others using the *+*. Specifically, now we
will add our data as points and we also get rid of the gray background.
```{r, echo=T}
ggplot(IP, aes(Plate, Log.CFU.gram, col = Site))+
  geom_jitter(width = 0.1)+ #this function spreads points along the x axis to avoid potential overlaps
  theme_classic() #just less gray and lines. see theme() in ggplot for more
```
We finally have our first data graph. It is important to look at this
plot and see that there are already from here interesting observation
that can be made. Some of them are explorative, other more inferential.

let's continue with

## the actual test

In the previous graph we have plotted all the data point we have
collected. However, we most likely want to report average values and the
inferential errors that are associated to them. With ggplot there is a
very smart way to do so. We can use the fucntion *stat_summary* that, as
the name suggests, can be use to summarise the actual statistics of the
plotted data.
```{r, echo=T}
ggplot(IP, aes(Plate, Log.CFU.gram, col = Site))+
  geom_point(position = position_dodge(width = 0.3), size = 2)+
  stat_summary(aes(fill = Site), fun.data = mean_se, geom = "pointrange", 
               position = position_dodge(width = 0.3), col = "black", pch= 21, size = 1)+
  theme_classic()
```
Here, we are plotting the averages of 4 data points (4 independent
measurements/students). The associated errors are SEM.

If we now call the previous run *model* and look at its summary we can
better understand what we are observing on the just obtained graph.
```{r, echo=T}
summary(model)
```
These results indicate that both variables **Plate** and **Site** and
their interaction contribute to the observed differences significantly.

We can now ask which of the different plates are characterized by a
significant difference between **Sites**. We can take single pairs and
test them individually. With some basic R knowledge this is extremely
easy and very helpful, also with more complex data set than ours. Now it
is very important to remember that the data are not normally
distributed. Thus, if we would run a *t-test* we will increase
considerably the chance of *Type-I Error*. We can instead use the
*Wilcoxon test* that handles non-parametric distributions.
```{r, echo=T, include=F}
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
WT = list()

for(i in unique(IP$Plate)){
  temp = IP[which(IP$Plate %in% i),]
  p.val = wilcox.test(temp$Log.CFU.gram ~ temp$Site, alternative = "two.sided")$p.value
  WT[[i]] = p.val
}
single.pairs = as.data.frame(do.call(rbind, WT))
colnames(single.pairs) = "p.value"

single.pairs
```
In all cases, there is always a significant difference between the two
sites A and B with the exception of the AB kanamycin.

We can increase the details of our analysis and ask for significant
differences across all pairwise comparisons too. To do so, we cab run a
*Wilcoxon Test* and correct the obtained *p-values* because of multiple
testing.

```{r, echo=T}
p.w.t = pairwise.wilcox.test(IP$Log.CFU.gram, paste(IP$Plate, IP$Site), paired = FALSE, p.adjust.method ="BH")$p.value

m.pwt = colorDF(melt(p.w.t, na.rm = T))
col_type(m.pwt, "value") <- "pval"

knitr::kable(m.pwt)
```
